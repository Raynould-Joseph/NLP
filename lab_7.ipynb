{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab 7",
      "provenance": [],
      "authorship_tag": "ABX9TyP2NUkyvAYugnPP7S11opbO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raynould-Joseph/NLP/blob/main/lab_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wTUSexB_JJfI",
        "outputId": "560990b8-d8ba-4e94-85ca-66b5cfeef7fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a name: raynould\n",
            "raynould\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "1xKB_kb5MoXs",
        "outputId": "6a5cbd0c-86a9-489c-82e7-12fd9a131574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word: rocks\n",
            "\n",
            "\n",
            "word entered: rocks\n",
            "\n",
            "\n",
            "After Lementization\n",
            "rocks rock\n",
            "\n",
            "\n",
            "Printing the corpora\n",
            "corpora : corpus\n",
            "\n",
            "\n",
            "rocks rocks\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "# import these modules\n",
        "#entering the word from the user\n",
        "word = input(\"Enter a word: \")\n",
        "print(\"\\n\")\n",
        "print(\"word entered:\",word)\n",
        "print(\"\\n\")\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "  \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"After Lementization\")\n",
        "print(word, lemmatizer.lemmatize(word))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Printing the corpora\")\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "print(\"\\n\")\n",
        "\n",
        "  \n",
        "# a denotes adjective in \"pos\"\n",
        "print(word, lemmatizer.lemmatize(word, pos =\"a\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lementisation using sentences"
      ],
      "metadata": {
        "id": "NXci-P-FGWA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define the sentence to be lemmatized\n",
        "sentence = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "# Tokenize: Split the sentence into words\n",
        "word_list = nltk.word_tokenize(sentence)\n",
        "print(word_list)\n",
        "#> ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
        "\n",
        "# Lemmatize list of words and join\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "print(lemmatized_output)\n",
        "#> The striped bat are hanging on their foot for best"
      ],
      "metadata": {
        "id": "XjNyXebHHDC6",
        "outputId": "b4b5677d-3a02-46b3-90cd-6446f28993b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
            "The striped bat are hanging on their foot for best\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wordnet Lemmatization and POS Tagging in Python"
      ],
      "metadata": {
        "id": "Jf65VjvKHyJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import defaultdict\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "\n",
        "text = \"guru99 is a totally new kind of learning experience.\"\n",
        "tokens = word_tokenize(text)\n",
        "lemma_function = WordNetLemmatizer()\n",
        "for tokens, tag in pos_tag(tokens):\n",
        "  lemma = lemma_function.lemmatize(tokens, tag_map[tag[0]])\n",
        "  print(tokens, \"=>\", lemma)"
      ],
      "metadata": {
        "id": "_QoOq5RCHzzR",
        "outputId": "f866dd18-0bc1-4232-d0eb-0769ec319183",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "guru99 => guru99\n",
            "is => be\n",
            "a => a\n",
            "totally => totally\n",
            "new => new\n",
            "kind => kind\n",
            "of => of\n",
            "learning => learn\n",
            "experience => experience\n",
            ". => .\n"
          ]
        }
      ]
    }
  ]
}