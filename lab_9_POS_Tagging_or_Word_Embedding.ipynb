{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNPepTbRbkfE3ip/Up0DGF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raynould-Joseph/NLP/blob/main/lab_9_POS_Tagging_or_Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. To Find the degree of similarity between two words"
      ],
      "metadata": {
        "id": "cNdARqH1S69G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GdVzEHB0BjVW",
        "outputId": "6fa0de98-81db-4052-cc81-05708c54291d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import word2vec\n",
        "corpus = [\n",
        "          'Text of the first document.',\n",
        "          'Text of the second document made longer.',\n",
        "          'Number three.',\n",
        "          'This is number four.',\n",
        "]\n",
        "# we need to pass splitted sentences to the model\n",
        "tokenized_sentences = [sentence.split() for sentence in corpus]\n",
        "model = word2vec.Word2Vec(tokenized_sentences, min_count=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe is also implemented with gensim library, its basic functionality to train on standard corpus is described with this snippet"
      ],
      "metadata": {
        "id": "Gj0gMLOITiOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from gensim.models.word2vec import Text8Corpus\n",
        "from glove import Corpus, Glove\n",
        "# sentences and corpus from standard library\n",
        "sentences = list(itertools.islice(Text8Corpus('text8'),None))\n",
        "corpus = Corpus()\n",
        "# fitting the corpus with sentences and creating Glove object\n",
        "corpus.fit(sentences, window=10)\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "# fitting to the corpus and adding standard dictionary to the object\n",
        "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)\n"
      ],
      "metadata": {
        "id": "BIhqJ_0oTjBY",
        "outputId": "0924216a-731d-4ecf-ca14-87bae8aa203c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ddc6d0603bdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText8Corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mglove\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# sentences and corpus from standard library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mText8Corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'glove'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk import RegexpParser\n"
      ],
      "metadata": {
        "id": "M94qTtTDT_Mo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "text =\"learn php from guru99 and make study easy\".split()\n",
        "print(\"After Split:\",text)\n",
        "tokens_tag = pos_tag(text)\n",
        "print(\"After Token:\",tokens_tag)\n",
        "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
        "chunker = RegexpParser(patterns)\n",
        "print(\"After Regex:\",chunker)\n",
        "output = chunker.parse(tokens_tag)\n",
        "print(\"After Chunking\",output)"
      ],
      "metadata": {
        "id": "NSxluwZOUImv",
        "outputId": "e3315a2e-a7e9-416c-b12b-8822964a6ac0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Split: ['learn', 'php', 'from', 'guru99', 'and', 'make', 'study', 'easy']\n",
            "After Token: [('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('guru99', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n",
            "After Regex: chunk.RegexpParser with 1 stages:\n",
            "RegexpChunkParser with 1 rules:\n",
            "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
            "After Chunking (S\n",
            "  (mychunk learn/JJ)\n",
            "  (mychunk php/NN)\n",
            "  from/IN\n",
            "  (mychunk guru99/NN and/CC)\n",
            "  make/VB\n",
            "  (mychunk study/NN easy/JJ))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zMIOcjVoS37b"
      }
    }
  ]
}